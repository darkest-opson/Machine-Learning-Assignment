{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7263637-2732-49f2-9772-c8de90c0b40a",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "A1. The KNN (K-Nearest Neighbors) algorithm is a non-parametric machine learning algorithm used for both classification and regression tasks. In the KNN algorithm, the input data is classified based on the k closest training data points in the feature space. For classification tasks, the class of the majority of the k-nearest neighbors is assigned to the input data point. For regression tasks, the output is the average of the values of the k-nearest neighbors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef95df5-c838-465f-9c35-a8e47b088f72",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "A2. The value of k in KNN is chosen based on the performance of the model on a validation set. A small value of k can lead to overfitting, while a large value of k can lead to underfitting. The value of k is generally chosen based on the value of accuracy or some other performance metric on the validation set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4f4818-82a9-42f5-8682-48b1e5f1733b",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "\n",
    "A3. The KNN classifier is used for classification tasks, while the KNN regressor is used for regression tasks. In the KNN classifier, the output is the majority class of the k-nearest neighbors, while in the KNN regressor, the output is the average of the values of the k-nearest neighbors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80956467-c928-4031-b4ea-17f0ef8f8c17",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "A4. The performance of KNN can be measured using various metrics such as accuracy, precision, recall, F1 score, and ROC-AUC score, depending on the task. Cross-validation can also be used to estimate the performance of the KNN algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acf47b5-aa31-4cc6-98c0-76f21a31828b",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "A5. The curse of dimensionality in KNN refers to the problem of having a high number of dimensions in the feature space, which can lead to a significant increase in the amount of data required to cover the space adequately. This can cause the KNN algorithm to become computationally expensive, as the number of possible data points grows exponentially with the dimensionality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff35aabb-17ca-4736-bded-ad92e489e973",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "A6. One way to handle missing values in KNN is to impute them using the mean, median, or mode of the non-missing values of the corresponding feature. Another approach is to use KNN imputation, where the missing values are imputed based on the values of the k-nearest neighbors in the feature space. Another method is to use algorithms that are specifically designed to handle missing values, such as decision trees and random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b86392e-3a21-430c-adcf-40ee8a6d737f",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "\n",
    "A7. The performance of the KNN classifier and regressor depends on the specific problem and the characteristics of the data. In general, the KNN classifier is better suited for classification problems, while the KNN regressor is better suited for regression problems. The KNN classifier can handle non-linear decision boundaries and is relatively simple to implement, but can be sensitive to the choice of k and the distance metric. The KNN regressor can handle non-linear relationships between the input and output variables, but can be sensitive to outliers and noisy data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c73f600-f8c1-4050-82fb-b1ba2a2b9268",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "\n",
    "A8. The strengths of the KNN algorithm for classification and regression tasks include its simplicity, ability to handle non-linear relationships, and flexibility in choosing the distance metric and value of k. The weaknesses of the KNN algorithm include its sensitivity to the choice of distance metric and value of k, its computational complexity, and its inability to handle high-dimensional data without dimensionality reduction. To address these weaknesses, techniques such as cross-validation, feature selection, and dimensionality reduction can be used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f49dca-13da-4c9e-933a-71349dab73b8",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "A9. The Euclidean distance and Manhattan distance are two commonly used distance metrics in KNN. Euclidean distance is the straight-line distance between two points in Euclidean space and is calculated as the square root of the sum of the squared differences between the corresponding coordinates. Manhattan distance, also known as city block distance or L1 distance, is the sum of the absolute differences between the corresponding coordinates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833bd245-b379-4483-acfd-6dacc75ba961",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "A10. Feature scaling is an important preprocessing step in KNN because it can affect the distance metric used to calculate the distances between data points. If the features have different scales or units, features with larger values may dominate the distance metric. Feature scaling can be done by scaling the features to have zero mean and unit variance (standardization) or by scaling the features to have a range between 0 and 1 (min-max scaling). This helps to ensure that all features contribute equally to the distance metric and improves the performance of the KNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6cdca-207d-4776-89ce-de7b48d29ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
