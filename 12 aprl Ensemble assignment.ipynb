{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ad9ade-8a9c-4cf0-9170-fb6035ed3344",
   "metadata": {},
   "source": [
    "Q1. How does bagging reduce overfitting in decision trees?\n",
    "\n",
    "Bagging, which stands for Bootstrap Aggregating, is a technique that reduces overfitting in decision trees by creating an ensemble of models that are trained on different subsets of the training data using a bootstrapping method. The bootstrapping method involves randomly selecting samples with replacement from the original training data to create multiple new training sets. By using different subsets of the data, the bagging algorithm creates a set of diverse models that are less likely to overfit to the training data. When making predictions, the bagging algorithm averages the predictions of all the models in the ensemble, which reduces the impact of any individual model that may have overfit to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4360c5c0-9c2d-419e-9d2d-09b56db73c1a",
   "metadata": {},
   "source": [
    "Q2. What are the advantages and disadvantages of using different types of base learners in bagging?\n",
    "\n",
    " The advantages and disadvantages of using different types of base learners in bagging can vary depending on the specific problem being solved. In general, using diverse base learners can lead to a more robust ensemble that is less prone to overfitting. However, using base learners that are too complex can lead to overfitting, while using base learners that are too simple can lead to underfitting. It is also important to consider the computational cost of using different types of base learners, as some algorithms may be more computationally expensive than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e779e4e9-a6f0-46a3-8944-8eb396bda955",
   "metadata": {},
   "source": [
    "Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?\n",
    "\n",
    "The choice of base learner can affect the bias-variance tradeoff in bagging. Bias refers to the error that is introduced by approximating a real-world problem with a simplified model, while variance refers to the error that is introduced by using a high-dimensional model that is overly sensitive to the noise in the training data. In bagging, using base learners with high bias and low variance (e.g., decision trees with low depth) can reduce the bias of the ensemble, while using base learners with high variance and low bias (e.g., deep neural networks) can reduce the variance of the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fb3398-7cb6-4b1c-9b96-06636ea8eede",
   "metadata": {},
   "source": [
    "Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?\n",
    "\n",
    "Bagging can be used for both classification and regression tasks. In classification tasks, the bagging algorithm creates an ensemble of models that predict the class labels of new instances based on the class labels of the training instances. In regression tasks, the bagging algorithm creates an ensemble of models that predict a continuous target variable based on the values of the training instances. The main difference between classification and regression in bagging is the way the predictions are combined, with classification typically using majority voting and regression using averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43cc5086-5ff3-475c-83a9-484ed99da891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `ensemble` not found.\n"
     ]
    }
   ],
   "source": [
    "Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3969c10-b805-48bb-9fc8-06b756835d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Can you provide an example of a real-world application of bagging in machine learning?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
