{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84525565-ad2b-4e96-ac88-e2d4a1e55793",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    " Gradient Boosting Regression is a machine learning algorithm that belongs to the boosting family of algorithms. It involves the iterative construction of a series of decision tree models, each attempting to correct the mistakes of the previous one, by fitting the residual errors in the prediction. In other words, the algorithm builds the model in a stepwise fashion, where each new model tries to improve the accuracy of the previous model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540cb072-0472-4906-9882-d235bf3238b9",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6b6d05c-bcb3-4bcc-afa3-0cab572bd14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    \n",
    "    def __init__(self, n_trees=100, max_depth=3, learning_rate=0.1):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trees = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.full(np.shape(y), np.mean(y))\n",
    "        for i in range(self.n_trees):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            residuals = y - y_pred\n",
    "            tree.fit(X, residuals)\n",
    "            update = tree.predict(X) * self.learning_rate\n",
    "            y_pred += update\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for tree in self.trees:\n",
    "            y_pred += tree.predict(X) * self.learning_rate\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6c087f2-f539-4a43-ae6c-343fe45580db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# generate sample data\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1)\n",
    "\n",
    "# split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# fit the gradient boosting model\n",
    "gbr = GradientBoostingRegressor(n_trees=100, max_depth=3, learning_rate=0.1)\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = gbr.predict(X_test)\n",
    "\n",
    "# evaluate the performance of the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73173472-ad73-4997-a6bd-0eae834fc0d7",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n",
    "\n",
    "Hyperparameters such as learning rate, number of trees, and tree depth can significantly affect the performance of the gradient boosting model. You can experiment with different hyperparameters to optimize the performance of the model. One way to do this is to use grid search or random search to find the best hyperparameters. Here's an example of how you can use grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca5b023e-d2da-42df-b170-539dc28e0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "data = load_boston()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [1, 2, 3, 4],\n",
    "}\n",
    "\n",
    "# Define the model\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "# Define the grid search object\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search object to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and corresponding mean squared error\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Best MSE: {-grid_search.best_score_}\")\n",
    "\n",
    "# Evaluate the model on the test set using the best hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Test MSE: {mse}\")\n",
    "print(f\"Test R^2: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb7609-d6af-49a7-ae5e-69db4afcaa81",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    " In Gradient Boosting, a weak learner is a simple machine learning model with low predictive power, such as a decision tree with a small number of splits. The idea is to build an ensemble of such weak learners that, when combined, can improve the overall performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb305d7f-c333-4381-8ecb-fcd492d4593b",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "The intuition behind Gradient Boosting is to iteratively fit a sequence of weak learners to the data, each one correcting the mistakes of the previous one. The idea is to build an ensemble of models that can capture complex patterns in the data by combining the individual strengths of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973b2a86-cd4a-4da2-9776-b20e0c4f6d82",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners by sequentially fitting a new model to the residuals of the previous model. At each iteration, the algorithm calculates the difference between the true target values and the predicted values of the current model. This difference, known as the residual, becomes the new target for the next model. The new model is then trained on the residuals, and its predictions are added to the predictions of the previous models, weighted by a learning rate parameter that controls the contribution of each model. This process is repeated until the desired number of models is reached or until the residuals converge to a minimum value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97061639-21b2-4bdf-90b1-33513f6ffcdf",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?\n",
    "\n",
    " The steps involved in constructing the mathematical intuition of Gradient Boosting algorithm are:\n",
    "\n",
    "Initialize the model with a constant value, usually the mean of the target variable.\n",
    "\n",
    "For each iteration:\n",
    "\n",
    "a. Calculate the negative gradient of the loss function with respect to the current predictions.\n",
    "\n",
    "b. Fit a weak learner, such as a decision tree, to the negative gradient residuals.\n",
    "\n",
    "c. Multiply the predictions of the new model by a learning rate parameter and add them to the predictions of the previous models.\n",
    "\n",
    "Repeat step 2 until the desired number of models is reached.\n",
    "\n",
    "Make predictions by combining the predictions of all models in the ensemble.\n",
    "\n",
    "Calculate the loss function on the predictions and the true targets to evaluate the performance of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
